The SEO Problem & How We Solved It
The Problem
Your site is a Single Page Application (SPA) built with React. SPAs work like this:

The server sends a tiny HTML shell (just ~1.3KB) with an empty <div id="root"></div>
JavaScript loads and builds all the content in the browser
Users see the full page after JavaScript runs
The issue: Search engines and AI crawlers (Google, Bing, ChatGPT, Claude, Perplexity) often don't run JavaScript. When they visit your site, they see:

<body>
  <a href="#main-content">Skip to main content</a>
  <div id="root"></div>  <!-- Empty! -->
</body>

That's it. No headings, no service descriptions, no testimonials, no keywords. Just an empty shell.

The Solution: Build-Time Pre-Rendering with Bot Detection
We implemented a two-part solution:

Part 1: Pre-render all 34 pages at build time
We created a script (scripts/prerender.ts) that:

Starts a local server with your built app
Launches a headless browser (Chromium via Puppeteer)
Visits every page on your site (all 34 routes)
Waits for JavaScript to fully render the content
Saves the complete HTML to files in dist/public/prerender/
For example:

/ → dist/public/prerender/index.html (72KB of full content)
/areas → dist/public/prerender/areas.html
/services/heat-pump-installation → dist/public/prerender/services/heating-and-heat-pump-installation.html
Part 2: Detect bots and serve them the pre-rendered HTML
We created middleware (server/botDetection.ts) that:

Checks every incoming request's User-Agent header
If it matches known crawlers (Googlebot, Bingbot, GPTBot, ChatGPT, Claude, etc.), serve the pre-rendered HTML file
If it's a regular browser, serve the normal SPA for the fast, interactive experience
Googlebot visits /areas
  → Bot detected! 
  → Serve dist/public/prerender/areas.html (56KB full HTML)
Chrome browser visits /areas
  → Regular user
  → Serve SPA shell (1.3KB), JavaScript builds the page

The Deployment Challenge
This worked locally, but failed during deployment because:

Puppeteer couldn't find a browser - It tries to download its own Chromium, but this doesn't work in Replit's production environment
The Nix environment requires system-installed Chromium - We had to install Chromium via the system packager, not npm
Puppeteer needed the exact path - We had to tell Puppeteer exactly where Chromium lives (/nix/store/.../chromium/bin/chromium)
Once we fixed those, the build runs during deployment, pre-renders all pages, and the bot detection serves them correctly.

The Result
Visitor	Response Size	What They See
Googlebot	72KB	Full HTML with all content, meta tags, structured data
ChatGPT	56KB	Full HTML with all content
Regular browser	1.3KB	Fast SPA that builds instantly
Your site now has the best of both worlds: great SEO for search engines and AI, plus a fast interactive experience for real users.